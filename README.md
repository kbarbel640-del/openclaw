# ğŸŒ¿ gclaw

[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](./LICENSE)

**ğŸŒ± A local-first AI agent gateway â€” defaults to Ollama, no API keys required.**

gclaw is a local-first fork of [OpenClaw](https://github.com/openclaw-ai/openclaw) that runs your own AI models on your own hardware with zero cloud dependencies. Built by [gthumb.ai](https://gthumb.ai) ğŸª´

## ğŸŒ¿ Why gclaw?

OpenClaw is great, but it defaults to cloud providers. gclaw flips the defaults so everything runs locally out of the box:

- ğŸŒ± **Ollama is the default provider** â€” not Anthropic
- ğŸŒ¿ **Onboarding wizard leads with local** â€” cloud is still there under "Advanced"
- ğŸƒ **Ships with local-first configs** â€” works out of the box with `ollama pull gemma3:4b`
- ğŸŒ² **Zero cloud dependencies** â€” no API keys, no accounts, no billing
- ğŸª´ **All cloud providers still work** â€” just not the default

See [FORK.md](./FORK.md) for the exact diff from upstream.

## ğŸŒ± Why local-first?

- ğŸ”’ **Privacy**: Your conversations never leave your machine
- ğŸ†“ **No API keys**: Get started in minutes with Ollama â€” no accounts, no billing
- ğŸŒ **Offline capable**: Works without internet once models are pulled
- ğŸ’š **Cost**: $0/month after hardware investment
- â˜ï¸ **Cloud fallback**: Cloud providers (Anthropic, OpenAI, etc.) still work when you want them

## ğŸš€ Quick Start

```bash
# 1. Install Ollama (if you haven't)
curl -fsSL https://ollama.com/install.sh | sh

# 2. Pull the default model
ollama pull gemma3:4b

# 3. Run the setup script
./scripts/setup-local.sh

# 4. Start gclaw
gclaw gateway start
```

Or use the interactive onboarding wizard:

```bash
gclaw onboard
```

The wizard defaults to Ollama/local models. Cloud providers are available under "Advanced" options.

## ğŸŒ¿ CLI Commands

| Command               | Description                             |
| --------------------- | --------------------------------------- |
| `gclaw gateway start` | Start the local agent gateway           |
| `gclaw onboard`       | Interactive setup wizard (Ollama-first) |
| `gclaw status`        | Check gateway and Ollama status         |
| `gclaw tui`           | TUI chat interface                      |

gclaw works fully offline once your Ollama models are pulled â€” no internet required.

## âš™ï¸ Configuration

Copy the example config and customize:

```bash
cp gclaw.example.json ~/.openclaw/config.json
```

See [gclaw.example.json](./gclaw.example.json) for all defaults.

### Default model: `ollama/gemma3:4b`

You can switch models anytime:

```bash
# Use a different Ollama model
ollama pull deepseek-coder-v2
# Then update your config's model.primary to "ollama/deepseek-coder-v2"

# Or switch to a cloud provider
# Set model.primary to "anthropic/claude-sonnet-4-5" and add your API key
```

### Supported Ollama Models

| RAM   | Model                   | Notes                            |
| ----- | ----------------------- | -------------------------------- |
| 8GB   | `llama3.3` (8B Q4)      | Default â€” good all-around        |
| 8GB   | `phi4-mini`             | Faster, lighter, good for coding |
| 8GB   | `qwen2.5:7b`            | Strong multilingual              |
| 16GB  | `deepseek-coder-v2:16b` | Best for code tasks              |
| 32GB+ | `llama3.1:70b-q4`       | Near cloud quality               |

See [docs/ollama-models.md](./docs/ollama-models.md) for the full guide.

## ğŸŒ¿ What's different from upstream OpenClaw?

See [FORK.md](./FORK.md) for a detailed changelog. In short:

- Default provider changed from Anthropic â†’ Ollama
- Onboarding wizard presents local/Ollama as the first option
- Model aliases include `local` and `llama` â†’ `ollama/gemma3:4b`
- Ships with local-first example config and setup script
- All cloud provider functionality is preserved â€” just not the default

## ğŸ“‹ Requirements

- **Node.js** â‰¥ 22
- **pnpm** (monorepo package manager)
- **Ollama** (for local inference) â€” [ollama.com](https://ollama.com)
- 8GB+ RAM recommended for llama3.3 (16GB+ for larger models)

## ğŸ› ï¸ Development

```bash
git clone https://github.com/GreenThumbMarket/gclaw.git
cd gclaw
pnpm install
pnpm build
pnpm test
```

See [CONTRIBUTING.md](./CONTRIBUTING.md) for more details.

## License

MIT â€” same as upstream OpenClaw. See [LICENSE](./LICENSE).
