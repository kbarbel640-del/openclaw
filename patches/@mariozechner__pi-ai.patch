diff --git a/dist/providers/openai-completions.js b/dist/providers/openai-completions.js
index a430c2240a37bcc7d05fb8e551c18964e7636c59..869eb60f0724c31b08aba2e2ee37da63d82696b2 100644
--- a/dist/providers/openai-completions.js
+++ b/dist/providers/openai-completions.js
@@ -231,20 +231,26 @@ function buildParams(model, context, options) {
         stream: true,
         stream_options: { include_usage: true },
     };
-    // Cerebras/xAI/Mistral dont like the "store" field
-    if (!model.baseUrl.includes("cerebras.ai") &&
-        !model.baseUrl.includes("api.x.ai") &&
-        !model.baseUrl.includes("mistral.ai") &&
-        !model.baseUrl.includes("chutes.ai")) {
+    // Only send OpenAI-specific params (like "store") to actual OpenAI models.
+    // For custom models via proxies like LiteLLM, check providerType.
+    // Fall back to baseUrl detection for known providers.
+    const isOpenAIModel = model.providerType === "openai" ||
+        (!model.providerType &&
+            !model.baseUrl.includes("cerebras.ai") &&
+            !model.baseUrl.includes("api.x.ai") &&
+            !model.baseUrl.includes("mistral.ai") &&
+            !model.baseUrl.includes("chutes.ai") &&
+            (model.baseUrl.includes("api.openai.com") || model.provider === "openai"));
+    if (isOpenAIModel) {
         params.store = false;
     }
     if (options?.maxTokens) {
-        // Mistral/Chutes uses max_tokens instead of max_completion_tokens
-        if (model.baseUrl.includes("mistral.ai") || model.baseUrl.includes("chutes.ai")) {
-            params.max_tokens = options?.maxTokens;
+        // Only OpenAI uses max_completion_tokens. Others (Mistral/Chutes/Anthropic via proxy) use max_tokens
+        if (isOpenAIModel) {
+            params.max_completion_tokens = options?.maxTokens;
         }
         else {
-            params.max_completion_tokens = options?.maxTokens;
+            params.max_tokens = options?.maxTokens;
         }
     }
     if (options?.temperature !== undefined) {
