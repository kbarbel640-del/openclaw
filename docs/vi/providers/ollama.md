---
summary: "Chạy OpenClaw với Ollama (môi trường chạy LLM cục bộ)"
read_when:
  - Bạn muốn chạy OpenClaw với các mô hình cục bộ thông qua Ollama
  - Bạn cần hướng dẫn thiết lập và cấu hình Ollama
title: "Ollama"
x-i18n:
  source_path: providers/ollama.md
  source_hash: 2992dd0a456d19c3
  provider: openai
  model: gpt-5.2-chat-latest
  workflow: v1
  generated_at: 2026-02-08T07:08:13Z
---

# Ollama

Ollama là một môi trường chạy LLM cục bộ giúp bạn dễ dàng chạy các mô hình mã nguồn mở trên máy của mình. OpenClaw tích hợp với API tương thích OpenAI của Ollama và có thể **tự động phát hiện các mô hình có khả năng dùng tool** khi bạn chọn tham gia với `OLLAMA_API_KEY` (hoặc auth profile) và không định nghĩa một mục `models.providers.ollama` rõ ràng.

## Khoi dong nhanh

1. Cài đặt Ollama: https://ollama.ai

2. Tải một mô hình:

```bash
ollama pull gpt-oss:20b
# or
ollama pull llama3.3
# or
ollama pull qwen2.5-coder:32b
# or
ollama pull deepseek-r1:32b
```

3. Bật Ollama cho OpenClaw (giá trị bất kỳ đều được; Ollama không yêu cầu khóa thật):

```bash
# Set environment variable
export OLLAMA_API_KEY="ollama-local"

# Or configure in your config file
openclaw config set models.providers.ollama.apiKey "ollama-local"
```

4. Sử dụng các mô hình Ollama:

```json5
{
  agents: {
    defaults: {
      model: { primary: "ollama/gpt-oss:20b" },
    },
  },
}
```

## Kham pha mo hinh (provider ngầm định)

Khi bạn đặt `OLLAMA_API_KEY` (hoặc auth profile) và **không** định nghĩa `models.providers.ollama`, OpenClaw sẽ khám phá các mô hình từ instance Ollama cục bộ tại `http://127.0.0.1:11434`:

- Truy vấn `/api/tags` và `/api/show`
- Chỉ giữ lại các mô hình báo cáo khả năng `tools`
- Đánh dấu `reasoning` khi mô hình báo cáo `thinking`
- Đọc `contextWindow` từ `model_info["<arch>.context_length"]` khi có
- Đặt `maxTokens` bằng 10× cửa sổ ngữ cảnh
- Đặt tất cả chi phí thành `0`

Cách này tránh phải khai báo mô hình thủ công trong khi vẫn giữ danh mục phù hợp với khả năng của Ollama.

Để xem các mô hình hiện có:

```bash
ollama list
openclaw models list
```

Để thêm một mô hình mới, chỉ cần tải nó bằng Ollama:

```bash
ollama pull mistral
```

Mô hình mới sẽ được tự động khám phá và sẵn sàng sử dụng.

Nếu bạn đặt `models.providers.ollama` một cách rõ ràng, việc tự động khám phá sẽ bị bỏ qua và bạn phải định nghĩa mô hình thủ công (xem bên dưới).

## Cau hinh

### Thiết lập cơ bản (khám phá ngầm định)

Cách đơn giản nhất để bật Ollama là thông qua biến môi trường:

```bash
export OLLAMA_API_KEY="ollama-local"
```

### Thiết lập rõ ràng (mô hình thủ công)

Sử dụng cấu hình rõ ràng khi:

- Ollama chạy trên host/port khác.
- Bạn muốn ép buộc cửa sổ ngữ cảnh hoặc danh sách mô hình cụ thể.
- Bạn muốn bao gồm các mô hình không báo cáo hỗ trợ tool.

```json5
{
  models: {
    providers: {
      ollama: {
        // Use a host that includes /v1 for OpenAI-compatible APIs
        baseUrl: "http://ollama-host:11434/v1",
        apiKey: "ollama-local",
        api: "openai-completions",
        models: [
          {
            id: "gpt-oss:20b",
            name: "GPT-OSS 20B",
            reasoning: false,
            input: ["text"],
            cost: { input: 0, output: 0, cacheRead: 0, cacheWrite: 0 },
            contextWindow: 8192,
            maxTokens: 8192 * 10
          }
        ]
      }
    }
  }
}
```

Nếu `OLLAMA_API_KEY` được đặt, bạn có thể bỏ qua `apiKey` trong mục provider và OpenClaw sẽ tự điền để kiểm tra khả dụng.

### Base URL tùy chỉnh (cấu hình rõ ràng)

Nếu Ollama đang chạy trên host hoặc port khác (cấu hình rõ ràng sẽ vô hiệu hóa tự động khám phá, vì vậy hãy định nghĩa mô hình thủ công):

```json5
{
  models: {
    providers: {
      ollama: {
        apiKey: "ollama-local",
        baseUrl: "http://ollama-host:11434/v1",
      },
    },
  },
}
```

### Chọn mô hình

Sau khi cấu hình, tất cả các mô hình Ollama của bạn đều khả dụng:

```json5
{
  agents: {
    defaults: {
      model: {
        primary: "ollama/gpt-oss:20b",
        fallbacks: ["ollama/llama3.3", "ollama/qwen2.5-coder:32b"],
      },
    },
  },
}
```

## Nang cao

### Mô hình suy luận

OpenClaw đánh dấu mô hình là có khả năng suy luận khi Ollama báo cáo `thinking` trong `/api/show`:

```bash
ollama pull deepseek-r1:32b
```

### Chi phí mô hình

Ollama miễn phí và chạy cục bộ, vì vậy tất cả chi phí mô hình đều được đặt là $0.

### Cấu hình Streaming

Do một [vấn đề đã biết](https://github.com/badlogic/pi-mono/issues/1205) trong SDK nền tảng với định dạng phản hồi của Ollama, **streaming bị tắt theo mặc định** cho các mô hình Ollama. Điều này ngăn các phản hồi bị hỏng khi sử dụng mô hình có khả năng dùng tool.

Khi streaming bị tắt, phản hồi sẽ được gửi một lần (chế độ không streaming), giúp tránh vấn đề khi các delta nội dung/suy luận xen kẽ gây ra đầu ra bị rối.

#### Bật lại Streaming (Nâng cao)

Nếu bạn muốn bật lại streaming cho Ollama (có thể gây vấn đề với mô hình có khả năng dùng tool):

```json5
{
  agents: {
    defaults: {
      models: {
        "ollama/gpt-oss:20b": {
          streaming: true,
        },
      },
    },
  },
}
```

#### Tắt Streaming cho các Provider khác

Bạn cũng có thể tắt streaming cho bất kỳ provider nào nếu cần:

```json5
{
  agents: {
    defaults: {
      models: {
        "openai/gpt-4": {
          streaming: false,
        },
      },
    },
  },
}
```

### Cửa sổ ngữ cảnh

Đối với các mô hình được tự động khám phá, OpenClaw sử dụng cửa sổ ngữ cảnh do Ollama báo cáo khi có, nếu không sẽ mặc định là `8192`. Bạn có thể ghi đè `contextWindow` và `maxTokens` trong cấu hình provider rõ ràng.

## Xu ly su co

### Không phát hiện Ollama

Hãy đảm bảo Ollama đang chạy và bạn đã đặt `OLLAMA_API_KEY` (hoặc auth profile), và rằng bạn **không** định nghĩa một mục `models.providers.ollama` rõ ràng:

```bash
ollama serve
```

Và đảm bảo API có thể truy cập được:

```bash
curl http://localhost:11434/api/tags
```

### Không có mô hình nào khả dụng

OpenClaw chỉ tự động khám phá các mô hình báo cáo hỗ trợ tool. Nếu mô hình của bạn không được liệt kê, hãy:

- Tải một mô hình có khả năng dùng tool, hoặc
- Định nghĩa mô hình rõ ràng trong `models.providers.ollama`.

Để thêm mô hình:

```bash
ollama list  # See what's installed
ollama pull gpt-oss:20b  # Pull a tool-capable model
ollama pull llama3.3     # Or another model
```

### Kết nối bị từ chối

Kiểm tra rằng Ollama đang chạy trên đúng port:

```bash
# Check if Ollama is running
ps aux | grep ollama

# Or restart Ollama
ollama serve
```

### Phản hồi bị hỏng hoặc xuất hiện tên tool trong đầu ra

Nếu bạn thấy các phản hồi bị rối chứa tên tool (như `sessions_send`, `memory_get`) hoặc văn bản bị phân mảnh khi dùng mô hình Ollama, đây là do vấn đề từ SDK phía trên với phản hồi streaming. **Điều này đã được khắc phục theo mặc định** trong phiên bản OpenClaw mới nhất bằng cách tắt streaming cho các mô hình Ollama.

Nếu bạn đã bật streaming thủ công và gặp vấn đề này:

1. Xóa cấu hình `streaming: true` khỏi các mục mô hình Ollama của bạn, hoặc
2. Đặt rõ ràng `streaming: false` cho các mô hình Ollama (xem [Streaming Configuration](#streaming-configuration))

## Xem them

- [Model Providers](/concepts/model-providers) - Tổng quan về tất cả các provider
- [Model Selection](/concepts/models) - Cách chọn mô hình
- [Configuration](/gateway/configuration) - Tham chiếu cấu hình đầy đủ
