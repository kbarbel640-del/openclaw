---
summary: "Eingehendes Bild-/Audio-/Videoâ€‘VerstÃ¤ndnis (optional) mit Anbieterâ€‘ und CLIâ€‘Fallbacks"
read_when:
  - Entwurf oder Refactoring des MedienverstÃ¤ndnisses
  - Feinabstimmung der eingehenden Audio-/Video-/Bildâ€‘Vorverarbeitung
title: "MedienverstÃ¤ndnis"
x-i18n:
  source_path: nodes/media-understanding.md
  source_hash: 4b275b152060eae3
  provider: openai
  model: gpt-5.2-chat-latest
  workflow: v1
  generated_at: 2026-02-08T07:05:03Z
---

# MedienverstÃ¤ndnis (eingehend) â€” 2026-01-17

OpenClaw kann **eingehende Medien zusammenfassen** (Bild/Audio/Video), bevor die Antwortâ€‘Pipeline startet. Es erkennt automatisch, ob lokale Werkzeuge oder Anbieterâ€‘SchlÃ¼ssel verfÃ¼gbar sind, und kann deaktiviert oder angepasst werden. Wenn das VerstÃ¤ndnis ausgeschaltet ist, erhalten Modelle weiterhin wie gewohnt die Originaldateien/URLs.

## Ziele

- Optional: eingehende Medien vorab in kurzen Text Ã¼berfÃ¼hren fÃ¼r schnelleres Routing + bessere Befehlsanalyse.
- Originale Medienzustellung an das Modell bewahren (immer).
- **Anbieterâ€‘APIs** und **CLIâ€‘Fallbacks** unterstÃ¼tzen.
- Mehrere Modelle mit geordneter Fallbackâ€‘Reihenfolge erlauben (Fehler/GrÃ¶ÃŸe/Timeout).

## Verhalten auf hoher Ebene

1. Eingehende AnhÃ¤nge sammeln (`MediaPaths`, `MediaUrls`, `MediaTypes`).
2. FÃ¼r jede aktivierte FÃ¤higkeit (Bild/Audio/Video) AnhÃ¤nge gemÃ¤ÃŸ Richtlinie auswÃ¤hlen (Standard: **erster**).
3. Den ersten geeigneten Modelletrag wÃ¤hlen (GrÃ¶ÃŸe + FÃ¤higkeit + Auth).
4. Wenn ein Modell fehlschlÃ¤gt oder das Medium zu groÃŸ ist, **auf den nÃ¤chsten Eintrag zurÃ¼ckfallen**.
5. Bei Erfolg:
   - `Body` wird zu einem `[Image]`â€‘, `[Audio]`â€‘ oder `[Video]`â€‘Block.
   - Audio setzt `{{Transcript}}`; die Befehlsanalyse nutzt den Captionâ€‘Text, falls vorhanden,
     andernfalls das Transkript.
   - Captions werden als `User text:` innerhalb des Blocks beibehalten.

Wenn das VerstÃ¤ndnis fehlschlÃ¤gt oder deaktiviert ist, **lÃ¤uft der Antwortfluss fort** mit dem ursprÃ¼nglichen Body + AnhÃ¤ngen.

## KonfigurationsÃ¼bersicht

`tools.media` unterstÃ¼tzt **gemeinsame Modelle** plus fÃ¤higkeitsspezifische Ãœberschreibungen:

- `tools.media.models`: gemeinsame Modellliste (verwenden Sie `capabilities` zur Steuerung).
- `tools.media.image` / `tools.media.audio` / `tools.media.video`:
  - Standards (`prompt`, `maxChars`, `maxBytes`, `timeoutSeconds`, `language`)
  - Anbieterâ€‘Ãœberschreibungen (`baseUrl`, `headers`, `providerOptions`)
  - Deepgramâ€‘Audiooptionen Ã¼ber `tools.media.audio.providerOptions.deepgram`
  - optionale **fÃ¤higkeitsspezifische `models`â€‘Liste** (bevorzugt vor gemeinsamen Modellen)
  - `attachments`â€‘Richtlinie (`mode`, `maxAttachments`, `prefer`)
  - `scope` (optionale Steuerung nach Kanal/Chatâ€‘Typ/SitzungsschlÃ¼ssel)
- `tools.media.concurrency`: maximale gleichzeitige FÃ¤higkeitslÃ¤ufe (Standard **2**).

```json5
{
  tools: {
    media: {
      models: [
        /* shared list */
      ],
      image: {
        /* optional overrides */
      },
      audio: {
        /* optional overrides */
      },
      video: {
        /* optional overrides */
      },
    },
  },
}
```

### ModelletrÃ¤ge

Jeder `models[]`â€‘Eintrag kann **Anbieter** oder **CLI** sein:

```json5
{
  type: "provider", // default if omitted
  provider: "openai",
  model: "gpt-5.2",
  prompt: "Describe the image in <= 500 chars.",
  maxChars: 500,
  maxBytes: 10485760,
  timeoutSeconds: 60,
  capabilities: ["image"], // optional, used for multiâ€‘modal entries
  profile: "vision-profile",
  preferredProfile: "vision-fallback",
}
```

```json5
{
  type: "cli",
  command: "gemini",
  args: [
    "-m",
    "gemini-3-flash",
    "--allowed-tools",
    "read_file",
    "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters.",
  ],
  maxChars: 500,
  maxBytes: 52428800,
  timeoutSeconds: 120,
  capabilities: ["video", "image"],
}
```

CLIâ€‘Vorlagen kÃ¶nnen auÃŸerdem verwenden:

- `{{MediaDir}}` (Verzeichnis, das die Mediendatei enthÃ¤lt)
- `{{OutputDir}}` (fÃ¼r diesen Lauf erstelltes Scratchâ€‘Verzeichnis)
- `{{OutputBase}}` (Scratchâ€‘Dateiâ€‘Basispfad, ohne Erweiterung)

## Standardwerte und Limits

Empfohlene Standardwerte:

- `maxChars`: **500** fÃ¼r Bild/Video (kurz, befehlsfreundlich)
- `maxChars`: **nicht gesetzt** fÃ¼r Audio (vollstÃ¤ndiges Transkript, sofern Sie kein Limit setzen)
- `maxBytes`:
  - Bild: **10MB**
  - Audio: **20MB**
  - Video: **50MB**

Regeln:

- Ãœberschreitet ein Medium `maxBytes`, wird dieses Modell Ã¼bersprungen und **das nÃ¤chste Modell versucht**.
- Gibt das Modell mehr als `maxChars` zurÃ¼ck, wird die Ausgabe gekÃ¼rzt.
- `prompt` ist standardmÃ¤ÃŸig ein einfaches â€žBeschreiben Sie das {media}.â€œ plus die `maxChars`â€‘Hinweise (nur Bild/Video).
- Wenn `<capability>.enabled: true`, aber keine Modelle konfiguriert sind, versucht OpenClaw das
  **aktive Antwortmodell**, sofern dessen Anbieter die FÃ¤higkeit unterstÃ¼tzt.

### Automatische Erkennung des MedienverstÃ¤ndnisses (Standard)

Wenn `tools.media.<capability>.enabled` **nicht** auf `false` gesetzt ist und Sie keine
Modelle konfiguriert haben, erkennt OpenClaw automatisch in dieser Reihenfolge und **stoppt bei der ersten funktionierenden Option**:

1. **Lokale CLIs** (nur Audio; falls installiert)
   - `sherpa-onnx-offline` (erfordert `SHERPA_ONNX_MODEL_DIR` mit Encoder/Decoder/Joiner/Tokens)
   - `whisper-cli` (`whisper-cpp`; verwendet `WHISPER_CPP_MODEL` oder das gebÃ¼ndelte Tinyâ€‘Modell)
   - `whisper` (Pythonâ€‘CLI; lÃ¤dt Modelle automatisch herunter)
2. **Geminiâ€‘CLI** (`gemini`) mit `read_many_files`
3. **Anbieterâ€‘SchlÃ¼ssel**
   - Audio: OpenAI â†’ Groq â†’ Deepgram â†’ Google
   - Bild: OpenAI â†’ Anthropic â†’ Google â†’ MiniMax
   - Video: Google

Um die automatische Erkennung zu deaktivieren, setzen Sie:

```json5
{
  tools: {
    media: {
      audio: {
        enabled: false,
      },
    },
  },
}
```

Hinweis: Die Erkennung von BinÃ¤rdateien erfolgt nach bestem BemÃ¼hen unter macOS/Linux/Windows; stellen Sie sicher, dass die CLI auf `PATH` liegt (wir erweitern `~`), oder setzen Sie einen expliziten CLIâ€‘Modelleintrag mit vollstÃ¤ndigem Befehlspfad.

## FÃ¤higkeiten (optional)

Wenn Sie `capabilities` setzen, wird der Eintrag nur fÃ¼r diese Medientypen ausgefÃ¼hrt. FÃ¼r gemeinsame
Listen kann OpenClaw Standardwerte ableiten:

- `openai`, `anthropic`, `minimax`: **Bild**
- `google` (Geminiâ€‘API): **Bild + Audio + Video**
- `groq`: **Audio**
- `deepgram`: **Audio**

FÃ¼r CLIâ€‘EintrÃ¤ge **setzen Sie `capabilities` explizit**, um Ã¼berraschende Zuordnungen zu vermeiden.
Wenn Sie `capabilities` weglassen, ist der Eintrag fÃ¼r die Liste zulÃ¤ssig, in der er erscheint.

## Anbieterâ€‘UnterstÃ¼tzungsmatrix (OpenClawâ€‘Integrationen)

| FÃ¤higkeit | Anbieterâ€‘Integration                              | Hinweise                                          |
| --------- | ------------------------------------------------- | ------------------------------------------------- |
| Bild      | OpenAI / Anthropic / Google / andere Ã¼ber `pi-ai` | Jedes bildfÃ¤hige Modell im Registry funktioniert. |
| Audio     | OpenAI, Groq, Deepgram, Google                    | Anbieterâ€‘Transkription (Whisper/Deepgram/Gemini). |
| Video     | Google (Geminiâ€‘API)                               | Anbieterâ€‘VideoverstÃ¤ndnis.                        |

## Empfohlene Anbieter

**Bild**

- Bevorzugen Sie Ihr aktives Modell, wenn es Bilder unterstÃ¼tzt.
- Gute Standardwerte: `openai/gpt-5.2`, `anthropic/claude-opus-4-6`, `google/gemini-3-pro-preview`.

**Audio**

- `openai/gpt-4o-mini-transcribe`, `groq/whisper-large-v3-turbo` oder `deepgram/nova-3`.
- CLIâ€‘Fallback: `whisper-cli` (whisperâ€‘cpp) oder `whisper`.
- Deepgramâ€‘Einrichtung: [Deepgram (Audioâ€‘Transkription)](/providers/deepgram).

**Video**

- `google/gemini-3-flash-preview` (schnell), `google/gemini-3-pro-preview` (umfangreicher).
- CLIâ€‘Fallback: `gemini`â€‘CLI (unterstÃ¼tzt `read_file` fÃ¼r Video/Audio).

## Anhangâ€‘Richtlinie

Die fÃ¤higkeitsspezifische `attachments` steuert, welche AnhÃ¤nge verarbeitet werden:

- `mode`: `first` (Standard) oder `all`
- `maxAttachments`: Begrenzung der Anzahl verarbeiteter Elemente (Standard **1**)
- `prefer`: `first`, `last`, `path`, `url`

Wenn `mode: "all"`, werden Ausgaben als `[Image 1/2]`, `[Audio 2/2]` usw. beschriftet.

## Konfigurationsbeispiele

### 1) Gemeinsame Modellliste + Ãœberschreibungen

```json5
{
  tools: {
    media: {
      models: [
        { provider: "openai", model: "gpt-5.2", capabilities: ["image"] },
        {
          provider: "google",
          model: "gemini-3-flash-preview",
          capabilities: ["image", "audio", "video"],
        },
        {
          type: "cli",
          command: "gemini",
          args: [
            "-m",
            "gemini-3-flash",
            "--allowed-tools",
            "read_file",
            "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters.",
          ],
          capabilities: ["image", "video"],
        },
      ],
      audio: {
        attachments: { mode: "all", maxAttachments: 2 },
      },
      video: {
        maxChars: 500,
      },
    },
  },
}
```

### 2) Nur Audio + Video (Bild aus)

```json5
{
  tools: {
    media: {
      audio: {
        enabled: true,
        models: [
          { provider: "openai", model: "gpt-4o-mini-transcribe" },
          {
            type: "cli",
            command: "whisper",
            args: ["--model", "base", "{{MediaPath}}"],
          },
        ],
      },
      video: {
        enabled: true,
        maxChars: 500,
        models: [
          { provider: "google", model: "gemini-3-flash-preview" },
          {
            type: "cli",
            command: "gemini",
            args: [
              "-m",
              "gemini-3-flash",
              "--allowed-tools",
              "read_file",
              "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters.",
            ],
          },
        ],
      },
    },
  },
}
```

### 3) Optionales BildverstÃ¤ndnis

```json5
{
  tools: {
    media: {
      image: {
        enabled: true,
        maxBytes: 10485760,
        maxChars: 500,
        models: [
          { provider: "openai", model: "gpt-5.2" },
          { provider: "anthropic", model: "claude-opus-4-6" },
          {
            type: "cli",
            command: "gemini",
            args: [
              "-m",
              "gemini-3-flash",
              "--allowed-tools",
              "read_file",
              "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters.",
            ],
          },
        ],
      },
    },
  },
}
```

### 4) Multimodaler Einzeleintrag (explizite FÃ¤higkeiten)

```json5
{
  tools: {
    media: {
      image: {
        models: [
          {
            provider: "google",
            model: "gemini-3-pro-preview",
            capabilities: ["image", "video", "audio"],
          },
        ],
      },
      audio: {
        models: [
          {
            provider: "google",
            model: "gemini-3-pro-preview",
            capabilities: ["image", "video", "audio"],
          },
        ],
      },
      video: {
        models: [
          {
            provider: "google",
            model: "gemini-3-pro-preview",
            capabilities: ["image", "video", "audio"],
          },
        ],
      },
    },
  },
}
```

## Statusausgabe

Wenn das MedienverstÃ¤ndnis ausgefÃ¼hrt wird, enthÃ¤lt `/status` eine kurze Zusammenfassungszeile:

```
ðŸ“Ž Media: image ok (openai/gpt-5.2) Â· audio skipped (maxBytes)
```

Dies zeigt pro FÃ¤higkeit die Ergebnisse sowie den gewÃ¤hlten Anbieter/das gewÃ¤hlte Modell, sofern zutreffend.

## Hinweise

- Das VerstÃ¤ndnis erfolgt **nach bestem BemÃ¼hen**. Fehler blockieren Antworten nicht.
- AnhÃ¤nge werden auch dann an Modelle weitergegeben, wenn das VerstÃ¤ndnis deaktiviert ist.
- Verwenden Sie `scope`, um einzuschrÃ¤nken, wo das VerstÃ¤ndnis ausgefÃ¼hrt wird (z.â€¯B. nur Direktnachrichten).

## Verwandte Dokumente

- [Konfiguration](/gateway/configuration)
- [Bildâ€‘ & MedienunterstÃ¼tzung](/nodes/images)
