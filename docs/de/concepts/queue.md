---
summary: "Design der Befehlswarteschlange, das eingehende Auto-Reply-Ausführungen serialisiert"
read_when:
  - "Ändern der Auto-Reply-Ausführung oder -Nebenläufigkeit"
title: "Befehlswarteschlange"
x-i18n:
  source_path: concepts/queue.md
  source_hash: 2104c24d200fb4f9
  provider: openai
  model: gpt-5.2-chat-latest
  workflow: v1
  generated_at: 2026-02-08T07:04:08Z
---

# Befehlswarteschlange (2026-01-16)

Wir serialisieren eingehende Auto-Reply-Ausführungen (alle Kanäle) über eine kleine In-Process-Warteschlange, um zu verhindern, dass mehrere Agentenläufe kollidieren, und ermöglichen gleichzeitig sichere Parallelität über Sitzungen hinweg.

## Warum

- Auto-Reply-Ausführungen können teuer sein (LLM-Aufrufe) und kollidieren, wenn mehrere eingehende Nachrichten kurz hintereinander eintreffen.
- Die Serialisierung vermeidet Konkurrenz um gemeinsam genutzte Ressourcen (Sitzungsdateien, Logs, CLI-stdin) und reduziert die Wahrscheinlichkeit von Upstream-Rate-Limits.

## Funktionsweise

- Eine spurabhängige FIFO-Warteschlange leert jede Spur mit einer konfigurierbaren Nebenläufigkeitsgrenze (Standard 1 für nicht konfigurierte Spuren; main standardmäßig 4, subagent 8).
- `runEmbeddedPiAgent` stellt nach **Sitzungsschlüssel** (Spur `session:<key>`) ein, um zu garantieren, dass pro Sitzung nur eine aktive Ausführung läuft.
- Jede Sitzungsausführung wird anschließend in eine **globale Spur** (standardmäßig `main`) eingereiht, sodass die Gesamtparallelität durch `agents.defaults.maxConcurrent` begrenzt ist.
- Wenn ausführliche Protokollierung aktiviert ist, geben eingereihte Ausführungen einen kurzen Hinweis aus, wenn sie vor dem Start länger als ~2 s warten mussten.
- Tippindikatoren werden weiterhin sofort beim Einreihen ausgelöst (sofern vom Kanal unterstützt), sodass sich die Nutzererfahrung während der Wartezeit nicht ändert.

## Warteschlangenmodi (pro Kanal)

Eingehende Nachrichten können die aktuelle Ausführung steuern, auf einen Folgezug warten oder beides:

- `steer`: sofort in die aktuelle Ausführung einspeisen (bricht ausstehende Werkzeugaufrufe nach der nächsten Werkzeug-Grenze ab). Falls nicht streamingfähig, Rückfall auf Folgezug.
- `followup`: für den nächsten Agentenzug nach Ende der aktuellen Ausführung einreihen.
- `collect`: alle eingereihten Nachrichten zu **einem einzigen** Folgezug zusammenführen (Standard). Wenn Nachrichten unterschiedliche Kanäle/Threads adressieren, werden sie einzeln abgearbeitet, um die Weiterleitung zu erhalten.
- `steer-backlog` (alias `steer+backlog`): jetzt steuern **und** die Nachricht für einen Folgezug aufbewahren.
- `interrupt` (Legacy): die aktive Ausführung für diese Sitzung abbrechen und dann die neueste Nachricht ausführen.
- `queue` (Legacy-Alias): identisch mit `steer`.

Steer-Backlog bedeutet, dass Sie nach der gesteuerten Ausführung eine Folgeantwort erhalten können; dadurch können Streaming-Oberflächen wie Duplikate wirken. Bevorzugen Sie `collect`/`steer`, wenn Sie
eine Antwort pro eingehender Nachricht wünschen.
Senden Sie `/queue collect` als eigenständigen Befehl (pro Sitzung) oder setzen Sie `messages.queue.byChannel.discord: "collect"`.

Standardwerte (wenn in der Konfiguration nicht gesetzt):

- Alle Oberflächen → `collect`

Global oder pro Kanal über `messages.queue` konfigurieren:

```json5
{
  messages: {
    queue: {
      mode: "collect",
      debounceMs: 1000,
      cap: 20,
      drop: "summarize",
      byChannel: { discord: "collect" },
    },
  },
}
```

## Warteschlangenoptionen

Optionen gelten für `followup`, `collect` und `steer-backlog` (und für `steer`, wenn auf Folgezug zurückgefallen wird):

- `debounceMs`: vor dem Start eines Folgezuges auf Ruhe warten (verhindert „continue, continue“).
- `cap`: maximale Anzahl eingereihter Nachrichten pro Sitzung.
- `drop`: Überlaufstrategie (`old`, `new`, `summarize`).

„Summarize“ behält eine kurze Stichpunktliste verworfener Nachrichten und injiziert sie als synthetischen Folge-Prompt.
Standardwerte: `debounceMs: 1000`, `cap: 20`, `drop: summarize`.

## Sitzungsbezogene Überschreibungen

- Senden Sie `/queue <mode>` als eigenständigen Befehl, um den Modus für die aktuelle Sitzung zu speichern.
- Optionen können kombiniert werden: `/queue collect debounce:2s cap:25 drop:summarize`
- `/queue default` oder `/queue reset` löscht die Sitzungsüberschreibung.

## Geltungsbereich und Garantien

- Gilt für Auto-Reply-Agentenläufe über alle eingehenden Kanäle, die die Gateway-Antwortpipeline verwenden (WhatsApp Web, Telegram, Slack, Discord, Signal, iMessage, Webchat usw.).
- Die Standardspur (`main`) ist prozessweit für eingehend + main-Heartbeats; setzen Sie `agents.defaults.maxConcurrent`, um mehrere Sitzungen parallel zuzulassen.
- Zusätzliche Spuren können existieren (z. B. `cron`, `subagent`), sodass Hintergrundjobs parallel laufen können, ohne eingehende Antworten zu blockieren.
- Sitzungsspezifische Spuren garantieren, dass zu einem Zeitpunkt nur eine Agentenausführung eine bestimmte Sitzung berührt.
- Keine externen Abhängigkeiten oder Hintergrund-Worker-Threads; reines TypeScript + Promises.

## Fehlerbehebung

- Wenn Befehle festzustecken scheinen, aktivieren Sie ausführliche Logs und suchen Sie nach Zeilen wie „queued for …ms“, um zu bestätigen, dass die Warteschlange abgearbeitet wird.
- Wenn Sie die Warteschlangentiefe benötigen, aktivieren Sie ausführliche Logs und beobachten Sie die Zeilen zur Warteschlangenzeit.
