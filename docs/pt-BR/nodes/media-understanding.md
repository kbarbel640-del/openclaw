---
summary: "Compreens√£o de imagens/√°udio/v√≠deo de entrada (opcional) com provedor + fallbacks de CLI"
read_when:
  - Projetando ou refatorando a compreens√£o de m√≠dia
  - Ajustando o pr√©-processamento de √°udio/v√≠deo/imagem de entrada
title: "Compreens√£o de M√≠dia"
x-i18n:
  source_path: nodes/media-understanding.md
  source_hash: 4b275b152060eae3
  provider: openai
  model: gpt-5.2-chat-latest
  workflow: v1
  generated_at: 2026-02-08T06:56:54Z
---

# Compreens√£o de M√≠dia (Entrada) ‚Äî 2026-01-17

O OpenClaw pode **resumir m√≠dias de entrada** (imagem/√°udio/v√≠deo) antes que o pipeline de resposta seja executado. Ele detecta automaticamente quando ferramentas locais ou chaves de provedor est√£o dispon√≠veis e pode ser desativado ou personalizado. Se a compreens√£o estiver desativada, os modelos ainda recebem os arquivos/URLs originais normalmente.

## Objetivos

- Opcional: pr√©-digerir m√≠dias de entrada em texto curto para roteamento mais r√°pido + melhor an√°lise de comandos.
- Preservar a entrega da m√≠dia original ao modelo (sempre).
- Suportar **APIs de provedores** e **fallbacks de CLI**.
- Permitir m√∫ltiplos modelos com fallback ordenado (erro/tamanho/timeout).

## Comportamento em alto n√≠vel

1. Coletar anexos de entrada (`MediaPaths`, `MediaUrls`, `MediaTypes`).
2. Para cada capacidade habilitada (imagem/√°udio/v√≠deo), selecionar anexos conforme a pol√≠tica (padr√£o: **primeiro**).
3. Escolher a primeira entrada de modelo eleg√≠vel (tamanho + capacidade + autentica√ß√£o).
4. Se um modelo falhar ou a m√≠dia for grande demais, **fazer fallback para a pr√≥xima entrada**.
5. Em caso de sucesso:
   - `Body` torna-se um bloco `[Image]`, `[Audio]` ou `[Video]`.
   - √Åudio define `{{Transcript}}`; a an√°lise de comandos usa o texto da legenda quando presente,
     caso contr√°rio, a transcri√ß√£o.
   - Legendas s√£o preservadas como `User text:` dentro do bloco.

Se a compreens√£o falhar ou estiver desativada, **o fluxo de resposta continua** com o corpo original + anexos.

## Vis√£o geral da configura√ß√£o

`tools.media` suporta **modelos compartilhados** al√©m de substitui√ß√µes por capacidade:

- `tools.media.models`: lista de modelos compartilhados (use `capabilities` para controlar).
- `tools.media.image` / `tools.media.audio` / `tools.media.video`:
  - padr√µes (`prompt`, `maxChars`, `maxBytes`, `timeoutSeconds`, `language`)
  - substitui√ß√µes por provedor (`baseUrl`, `headers`, `providerOptions`)
  - op√ß√µes de √°udio do Deepgram via `tools.media.audio.providerOptions.deepgram`
  - lista **`models` por capacidade** opcional (preferida antes dos modelos compartilhados)
  - pol√≠tica `attachments` (`mode`, `maxAttachments`, `prefer`)
  - `scope` (controle opcional por canal/chatType/chave de sess√£o)
- `tools.media.concurrency`: m√°ximo de execu√ß√µes concorrentes por capacidade (padr√£o **2**).

```json5
{
  tools: {
    media: {
      models: [
        /* shared list */
      ],
      image: {
        /* optional overrides */
      },
      audio: {
        /* optional overrides */
      },
      video: {
        /* optional overrides */
      },
    },
  },
}
```

### Entradas de modelo

Cada entrada `models[]` pode ser **provedor** ou **CLI**:

```json5
{
  type: "provider", // default if omitted
  provider: "openai",
  model: "gpt-5.2",
  prompt: "Describe the image in <= 500 chars.",
  maxChars: 500,
  maxBytes: 10485760,
  timeoutSeconds: 60,
  capabilities: ["image"], // optional, used for multi‚Äëmodal entries
  profile: "vision-profile",
  preferredProfile: "vision-fallback",
}
```

```json5
{
  type: "cli",
  command: "gemini",
  args: [
    "-m",
    "gemini-3-flash",
    "--allowed-tools",
    "read_file",
    "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters.",
  ],
  maxChars: 500,
  maxBytes: 52428800,
  timeoutSeconds: 120,
  capabilities: ["video", "image"],
}
```

Modelos de CLI tamb√©m podem usar:

- `{{MediaDir}}` (diret√≥rio que cont√©m o arquivo de m√≠dia)
- `{{OutputDir}}` (diret√≥rio tempor√°rio criado para esta execu√ß√£o)
- `{{OutputBase}}` (caminho base do arquivo tempor√°rio, sem extens√£o)

## Padr√µes e limites

Padr√µes recomendados:

- `maxChars`: **500** para imagem/v√≠deo (curto, amig√°vel a comandos)
- `maxChars`: **n√£o definido** para √°udio (transcri√ß√£o completa, a menos que voc√™ defina um limite)
- `maxBytes`:
  - imagem: **10MB**
  - √°udio: **20MB**
  - v√≠deo: **50MB**

Regras:

- Se a m√≠dia exceder `maxBytes`, esse modelo √© ignorado e o **pr√≥ximo modelo √© tentado**.
- Se o modelo retornar mais que `maxChars`, a sa√≠da √© aparada.
- `prompt` usa por padr√£o o simples ‚ÄúDescribe the {media}.‚Äù mais a orienta√ß√£o `maxChars` (apenas imagem/v√≠deo).
- Se `<capability>.enabled: true` mas nenhum modelo estiver configurado, o OpenClaw tenta o
  **modelo de resposta ativo** quando o provedor dele suporta a capacidade.

### Detec√ß√£o autom√°tica de compreens√£o de m√≠dia (padr√£o)

Se `tools.media.<capability>.enabled` **n√£o** estiver definido como `false` e voc√™ n√£o tiver
modelos configurados, o OpenClaw detecta automaticamente nesta ordem e **para na primeira
op√ß√£o funcional**:

1. **CLIs locais** (apenas √°udio; se instaladas)
   - `sherpa-onnx-offline` (requer `SHERPA_ONNX_MODEL_DIR` com encoder/decoder/joiner/tokens)
   - `whisper-cli` (`whisper-cpp`; usa `WHISPER_CPP_MODEL` ou o modelo tiny empacotado)
   - `whisper` (CLI em Python; baixa modelos automaticamente)
2. **Gemini CLI** (`gemini`) usando `read_many_files`
3. **Chaves de provedor**
   - √Åudio: OpenAI ‚Üí Groq ‚Üí Deepgram ‚Üí Google
   - Imagem: OpenAI ‚Üí Anthropic ‚Üí Google ‚Üí MiniMax
   - V√≠deo: Google

Para desativar a detec√ß√£o autom√°tica, defina:

```json5
{
  tools: {
    media: {
      audio: {
        enabled: false,
      },
    },
  },
}
```

Observa√ß√£o: a detec√ß√£o de bin√°rios √© best‚Äëeffort em macOS/Linux/Windows; garanta que a CLI esteja no `PATH` (expandimos `~`), ou defina um modelo de CLI expl√≠cito com um caminho completo do comando.

## Capacidades (opcional)

Se voc√™ definir `capabilities`, a entrada s√≥ roda para esses tipos de m√≠dia. Para listas
compartilhadas, o OpenClaw pode inferir padr√µes:

- `openai`, `anthropic`, `minimax`: **imagem**
- `google` (API Gemini): **imagem + √°udio + v√≠deo**
- `groq`: **√°udio**
- `deepgram`: **√°udio**

Para entradas de CLI, **defina `capabilities` explicitamente** para evitar correspond√™ncias inesperadas.
Se voc√™ omitir `capabilities`, a entrada √© eleg√≠vel para a lista em que aparece.

## Matriz de suporte de provedores (integra√ß√µes OpenClaw)

| Capacidade | Integra√ß√£o de provedor                           | Observa√ß√µes                                                    |
| ---------- | ------------------------------------------------ | -------------------------------------------------------------- |
| Imagem     | OpenAI / Anthropic / Google / outros via `pi-ai` | Qualquer modelo com capacidade de imagem no registro funciona. |
| √Åudio      | OpenAI, Groq, Deepgram, Google                   | Transcri√ß√£o pelo provedor (Whisper/Deepgram/Gemini).           |
| V√≠deo      | Google (API Gemini)                              | Compreens√£o de v√≠deo pelo provedor.                            |

## Provedores recomendados

**Imagem**

- Prefira seu modelo ativo se ele suportar imagens.
- Bons padr√µes: `openai/gpt-5.2`, `anthropic/claude-opus-4-6`, `google/gemini-3-pro-preview`.

**√Åudio**

- `openai/gpt-4o-mini-transcribe`, `groq/whisper-large-v3-turbo` ou `deepgram/nova-3`.
- Fallback de CLI: `whisper-cli` (whisper-cpp) ou `whisper`.
- Configura√ß√£o do Deepgram: [Deepgram (transcri√ß√£o de √°udio)](/providers/deepgram).

**V√≠deo**

- `google/gemini-3-flash-preview` (r√°pido), `google/gemini-3-pro-preview` (mais rico).
- Fallback de CLI: CLI `gemini` (suporta `read_file` em v√≠deo/√°udio).

## Pol√≠tica de anexos

A pol√≠tica `attachments` por capacidade controla quais anexos s√£o processados:

- `mode`: `first` (padr√£o) ou `all`
- `maxAttachments`: limita a quantidade processada (padr√£o **1**)
- `prefer`: `first`, `last`, `path`, `url`

Quando `mode: "all"`, as sa√≠das s√£o rotuladas como `[Image 1/2]`, `[Audio 2/2]`, etc.

## Exemplos de configura√ß√£o

### 1) Lista de modelos compartilhados + substitui√ß√µes

```json5
{
  tools: {
    media: {
      models: [
        { provider: "openai", model: "gpt-5.2", capabilities: ["image"] },
        {
          provider: "google",
          model: "gemini-3-flash-preview",
          capabilities: ["image", "audio", "video"],
        },
        {
          type: "cli",
          command: "gemini",
          args: [
            "-m",
            "gemini-3-flash",
            "--allowed-tools",
            "read_file",
            "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters.",
          ],
          capabilities: ["image", "video"],
        },
      ],
      audio: {
        attachments: { mode: "all", maxAttachments: 2 },
      },
      video: {
        maxChars: 500,
      },
    },
  },
}
```

### 2) Apenas √Åudio + V√≠deo (imagem desligada)

```json5
{
  tools: {
    media: {
      audio: {
        enabled: true,
        models: [
          { provider: "openai", model: "gpt-4o-mini-transcribe" },
          {
            type: "cli",
            command: "whisper",
            args: ["--model", "base", "{{MediaPath}}"],
          },
        ],
      },
      video: {
        enabled: true,
        maxChars: 500,
        models: [
          { provider: "google", model: "gemini-3-flash-preview" },
          {
            type: "cli",
            command: "gemini",
            args: [
              "-m",
              "gemini-3-flash",
              "--allowed-tools",
              "read_file",
              "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters.",
            ],
          },
        ],
      },
    },
  },
}
```

### 3) Compreens√£o opcional de imagem

```json5
{
  tools: {
    media: {
      image: {
        enabled: true,
        maxBytes: 10485760,
        maxChars: 500,
        models: [
          { provider: "openai", model: "gpt-5.2" },
          { provider: "anthropic", model: "claude-opus-4-6" },
          {
            type: "cli",
            command: "gemini",
            args: [
              "-m",
              "gemini-3-flash",
              "--allowed-tools",
              "read_file",
              "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters.",
            ],
          },
        ],
      },
    },
  },
}
```

### 4) Entrada √∫nica multimodal (capacidades expl√≠citas)

```json5
{
  tools: {
    media: {
      image: {
        models: [
          {
            provider: "google",
            model: "gemini-3-pro-preview",
            capabilities: ["image", "video", "audio"],
          },
        ],
      },
      audio: {
        models: [
          {
            provider: "google",
            model: "gemini-3-pro-preview",
            capabilities: ["image", "video", "audio"],
          },
        ],
      },
      video: {
        models: [
          {
            provider: "google",
            model: "gemini-3-pro-preview",
            capabilities: ["image", "video", "audio"],
          },
        ],
      },
    },
  },
}
```

## Sa√≠da de status

Quando a compreens√£o de m√≠dia √© executada, `/status` inclui uma linha curta de resumo:

```
üìé Media: image ok (openai/gpt-5.2) ¬∑ audio skipped (maxBytes)
```

Isso mostra os resultados por capacidade e o provedor/modelo escolhido quando aplic√°vel.

## Observa√ß√µes

- A compreens√£o √© **best‚Äëeffort**. Erros n√£o bloqueiam respostas.
- Anexos ainda s√£o passados aos modelos mesmo quando a compreens√£o est√° desativada.
- Use `scope` para limitar onde a compreens√£o √© executada (por exemplo, apenas Mensagens diretas).

## Documentos relacionados

- [Configura√ß√£o](/gateway/configuration)
- [Suporte a Imagens e M√≠dia](/nodes/images)
