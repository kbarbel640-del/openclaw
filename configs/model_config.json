{
  "$schema": "https://open-claw.bot/schemas/openclaw-models.json",
  "_comment": [
    "Model provider configuration for Brock agent stack.",
    "Four tiers: local Ollama (fast/free), z.ai API (strong/coding), OpenAI (backup), Anthropic (optional).",
    "This file is a reference config â€” merge into your openclaw.json under 'models' and 'agents.defaults'.",
    "Full strategy rationale: see CLOUD_STRATEGY.md in the repo root."
  ],

  "models": {
    "mode": "merge",
    "providers": {

      "local": {
        "baseUrl": "http://localhost:11434/v1",
        "api": "ollama",
        "auth": "api-key",
        "apiKey": "",
        "headers": {},
        "models": [
          {
            "id": "glm-5",
            "name": "GLM-5 (local)",
            "reasoning": true,
            "input": ["text"],
            "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
            "contextWindow": 131072,
            "maxTokens": 8192,
            "compat": {}
          }
        ]
      },

      "zai": {
        "baseUrl": "https://api.z.ai/api/coding/paas/v4",
        "api": "openai-completions",
        "auth": "api-key",
        "apiKey": "${ZAI_API_KEY}",
        "models": [
          {
            "id": "glm-5",
            "name": "GLM-5 (z.ai Coding)",
            "reasoning": true,
            "input": ["text", "image"],
            "cost": { "input": 0.002, "output": 0.006, "cacheRead": 0.001, "cacheWrite": 0.002 },
            "contextWindow": 131072,
            "maxTokens": 16384
          },
          {
            "id": "glm-4.7",
            "name": "GLM-4.7 (z.ai Coding)",
            "reasoning": true,
            "input": ["text", "image"],
            "cost": { "input": 0.001, "output": 0.003, "cacheRead": 0.0005, "cacheWrite": 0.001 },
            "contextWindow": 131072,
            "maxTokens": 16384
          }
        ]
      },

      "openai": {
        "baseUrl": "https://api.openai.com/v1",
        "api": "openai-completions",
        "auth": "api-key",
        "apiKey": "${OPENAI_API_KEY}",
        "models": [
          {
            "id": "gpt-4o",
            "name": "GPT-4o (OpenAI backup)",
            "reasoning": false,
            "input": ["text", "image"],
            "cost": { "input": 0.0025, "output": 0.01, "cacheRead": 0.00125, "cacheWrite": 0.0025 },
            "contextWindow": 128000,
            "maxTokens": 16384
          },
          {
            "id": "o3",
            "name": "o3 (OpenAI reasoning)",
            "reasoning": true,
            "input": ["text", "image"],
            "cost": { "input": 0.01, "output": 0.04, "cacheRead": 0.005, "cacheWrite": 0.01 },
            "contextWindow": 200000,
            "maxTokens": 100000
          }
        ]
      },

      "anthropic": {
        "baseUrl": "https://api.anthropic.com",
        "api": "anthropic-messages",
        "auth": "api-key",
        "apiKey": "${ANTHROPIC_API_KEY}",
        "_comment": "Optional. Uncomment in openclaw.json if you have a Pro/Max subscription. Strong long-context and prompt-injection resistance.",
        "models": [
          {
            "id": "claude-opus-4-6",
            "name": "Claude Opus 4.6 (Anthropic)",
            "reasoning": true,
            "input": ["text", "image"],
            "cost": { "input": 0.015, "output": 0.075, "cacheRead": 0.0075, "cacheWrite": 0.015 },
            "contextWindow": 200000,
            "maxTokens": 32000
          },
          {
            "id": "claude-sonnet-4-6",
            "name": "Claude Sonnet 4.6 (Anthropic)",
            "reasoning": true,
            "input": ["text", "image"],
            "cost": { "input": 0.003, "output": 0.015, "cacheRead": 0.0015, "cacheWrite": 0.003 },
            "contextWindow": 200000,
            "maxTokens": 16384
          }
        ]
      }
    }
  },

  "agents": {
    "defaults": {
      "model": {
        "primary": "zai/glm-5",
        "fallbacks": [
          "zai/glm-4.7",
          "openai/gpt-4o",
          "local/glm-5"
        ]
      },
      "imageModel": {
        "primary": "zai/glm-5",
        "fallbacks": ["openai/gpt-4o", "anthropic/claude-sonnet-4-6"]
      },
      "models": {
        "zai/glm-5": {
          "alias": "glm5",
          "streaming": true,
          "params": {}
        },
        "local/glm-5": {
          "alias": "local",
          "streaming": true,
          "params": {}
        },
        "openai/gpt-4o": {
          "alias": "gpt",
          "streaming": true,
          "params": {}
        },
        "openai/o3": {
          "alias": "o3",
          "streaming": true,
          "params": {}
        },
        "anthropic/claude-opus-4-6": {
          "alias": "opus",
          "streaming": true,
          "params": {}
        },
        "anthropic/claude-sonnet-4-6": {
          "alias": "sonnet",
          "streaming": true,
          "params": {}
        }
      }
    }
  },

  "_routing_notes": "See CLOUD_STRATEGY.md for full tier descriptions, failover logic, cost analysis, and the openclaw/lobsterBucket boundary.",

  "_soul_md_note": "SOUL.md was written for Claude. For local models (especially smaller ones), persona instructions may need to be more direct and repeated. If Brock drifts back to generic assistant behavior on local, try: (1) repeating key behavioral rules in the system prompt, (2) adding negative examples ('DO NOT say Great question'), (3) using shorter, more imperative phrasing. The z.ai GLM models tend to follow persona directives well."
}
